{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Define common functions used across different models and datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e394784635e1a8a"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configure the logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T17:37:14.435449100Z",
     "start_time": "2024-06-17T17:37:14.410111600Z"
    }
   },
   "id": "550728b874e3f12d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper methods"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "185b4eedd22b4552"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def calculate_spectrogram_dimensions(sample_length, n_fft, hop_length, sr=16000):\n",
    "    \"\"\"\n",
    "    Calculate the dimensions of the spectrogram without computing it.\n",
    "\n",
    "    Parameters:\n",
    "    samplerate (int): Sample rate of the audio file.\n",
    "    sample_length (int): Length of the audio sample in seconds.\n",
    "    n_fft (int): Number of FFT points.\n",
    "    hop_length (int): Number of samples between successive frames.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Dimensions of the spectrogram (frequency_bins, time_frames)\n",
    "    \"\"\"\n",
    "    # Total number of samples in the audio\n",
    "    total_samples = sr * sample_length\n",
    "    \n",
    "    # Number of frequency bins\n",
    "    frequency_bins = n_fft // 2 + 1\n",
    "    \n",
    "    # Number of time frames\n",
    "    time_frames = 1 + (total_samples - n_fft) // hop_length\n",
    "    \n",
    "    return frequency_bins, time_frames"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-01T10:37:41.941538700Z",
     "start_time": "2024-08-01T10:37:41.897675100Z"
    }
   },
   "id": "97d3db929bc29df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract specific features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dabfdf9d61b8ca2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "\n",
    "def extract_MFCCs(y, sr, n_fft, hop_length, n_mfcc=128, show_plt=False) -> np.ndarray:    \n",
    "    # MFCC — Mel-Frequency Cepstral Coefficients\n",
    "    # This feature is one of the most important method to extract a feature of an audio signal and is used majorly whenever working on audio signals. The mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope.\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    if show_plt:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.specshow(mfccs, sr=sr, x_axis='time', y_axis='mel', fmax=8000)\n",
    "        plt.title('MFCC')\n",
    "        plt.colorbar()\n",
    "        plt.show() \n",
    "    \n",
    "    return mfccs\n",
    "    \n",
    "def extract_stft_spectrogram(y, sr, hop_length, show_plt: bool,  n_fft=2048) -> np.ndarray:\n",
    "    # Short Term Fourier Transform (STFT) converts signal such that we can know the amplitude of given frequency at a given time. \n",
    "    stft = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    # You can think of a spectrogram as a bunch of FFTs stacked on top of each other. It is a way to visually represent a signal’s loudness, or amplitude, as it varies over time at different frequencies. There are some additional details going on behind the scenes when computing the spectrogram. The y-axis is converted to a log scale, and the color dimension is converted to decibels (you can think of this as the log scale of the amplitude). This is because humans can only perceive a very small and concentrated range of frequencies and amplitudes.\n",
    "    spect = np.abs(stft)\n",
    "    spect = librosa.amplitude_to_db(spect, ref=np.max) # Convert an amplitude spectrogram to dB-scaled spectrogram.\n",
    "    \n",
    "    if show_plt:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.specshow(spect, sr=sr, x_axis='time', y_axis='log')\n",
    "        plt.title('Spectrogram')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.show() \n",
    "        \n",
    "    return spect\n",
    "\n",
    "def extract_mel_spectrogram(y, sr, hop_length, show_plt, n_fft=2048, n_mels=128) -> np.ndarray:\n",
    "    # We are better at detecting differences in lower frequencies than higher frequencies.\n",
    "    # A mel spectrogram is a spectrogram where the frequencies are converted to the mel scale.\n",
    "    \n",
    "    # Where-as the mel-spectrogram has mel filters applied which reduces the number of bands to n_mels (typically 32-128)\n",
    "    mel_spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    mel_spect = librosa.power_to_db(mel_spect, ref=np.max) # moze ref=np.min\n",
    "    \n",
    "    if show_plt:\n",
    "        plt.figure(figsize=(14, 5))\n",
    "        librosa.display.specshow(mel_spect, sr=sr, y_axis='mel', fmax=8000, x_axis='time')\n",
    "        plt.title('Mel Spectrogram')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.show()\n",
    "        \n",
    "    return mel_spect\n",
    "\n",
    "def extract_chroma_stft(y, sr, hop_length=2048, n_fft=2048, n_chroma=128) -> np.ndarray:\n",
    "    stft = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))**2\n",
    "    chroma_stft = librosa.feature.chroma_stft(S=stft, n_chroma=n_chroma, sr=sr)\n",
    "    \n",
    "    return chroma_stft\n",
    "\n",
    "def extract_spectral_contrast(y, sr, hop_length=2048, n_fft=2048) -> np.ndarray:\n",
    "    stft = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "    spec_contr = librosa.feature.spectral_contrast(S=stft, sr=sr)\n",
    "    \n",
    "    return spec_contr\n",
    "\n",
    "    \n",
    "def print_waveform(y):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(y)\n",
    "    plt.title('Signal')\n",
    "    plt.xlabel('Time (samples)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T11:43:54.192790900Z",
     "start_time": "2024-08-04T11:43:53.797763700Z"
    }
   },
   "id": "e76a77fa1f8b679b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Extract all features"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce39964cac68a1f1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(samples, feature=\"Mel\", sr=16000, hop_length=2048, n_fft=2048, n_mels=128, n_chroma=128, n_mfcc=128, show_logs=False, show_plt=False) -> np.ndarray:\n",
    "    \"\"\"Extracts features from audio samples.\n",
    "\n",
    "    Args:\n",
    "        samples: List of audio samples.\n",
    "        sr: Sample rate (default: 16000 Hz).\n",
    "        debug: Enable debug logging (default: False).\n",
    "        show_plt: Whether to show matplotlib plot for feature\n",
    "    Returns:\n",
    "        Extracted features as a NumPy array.\n",
    "    \"\"\"\n",
    "    \n",
    "    features_length = len(samples)\n",
    "    features = []\n",
    "\n",
    "    for i, data in enumerate(samples):\n",
    "        if feature == \"Stft\":\n",
    "            spect = extract_stft_spectrogram(y=data, sr=sr, hop_length=hop_length, show_plt=show_plt, n_fft=n_fft)\n",
    "        elif feature == \"Mel\":\n",
    "            spect = extract_mel_spectrogram(y=data, sr=sr, hop_length=hop_length, show_plt=show_plt, n_fft=n_fft, n_mels=n_mels)    \n",
    "        elif feature == \"MFCC\":\n",
    "            spect = extract_MFCCs(y=data, sr=sr, show_plt=show_plt, n_mfcc=n_mfcc, hop_length=hop_length, n_fft=n_fft)          \n",
    "        elif feature == \"ChromaStft\":\n",
    "            spect = extract_chroma_stft(y=data, sr=sr,hop_length=hop_length, n_fft=n_fft, n_chroma=n_chroma)\n",
    "        elif feature == \"SpectralContrast\":\n",
    "            spect = extract_spectral_contrast(y=data, sr=sr, hop_length=hop_length, n_fft=n_fft)    \n",
    "            \n",
    "        features.append(spect)\n",
    "\n",
    "        if show_logs and i % 50 == 0:\n",
    "            logging.info(f\"Extracted {i}/{features_length} features.\")\n",
    "\n",
    "    features = np.array(features)\n",
    "\n",
    "    if show_logs:\n",
    "        logging.info(f\"Extracted features from {features_length} audio files.\")\n",
    "        logging.info(f\"Features shape: {features.shape}\")\n",
    "\n",
    "    return features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T19:21:54.222357400Z",
     "start_time": "2024-07-04T19:21:54.117326700Z"
    }
   },
   "id": "d35933c5d1115d84"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Onehot encode"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b65258c6be0cddc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Perform one-hot encoding for the labels convert to one-valued arr\n",
    "def onehot_encode(labels: np.ndarray) -> (np.ndarray, dict):\n",
    "    print(\"[INFO] Performing one-hot encoding on multiple nodes...\")\n",
    "    # Version 2 -> n nodes\n",
    "    encoder = OneHotEncoder()\n",
    "    labels = np.array(labels).reshape(-1, 1)\n",
    "    labels_onehot = encoder.fit_transform(labels)\n",
    "    labels_onehot = labels_onehot.astype(float) # convert to float\n",
    "    labels_onehot = labels_onehot.toarray()\n",
    "    \n",
    "    # Display the mapping between original classes and one-hot encoded columns\n",
    "    original_classes = encoder.categories_[0]\n",
    "    mapping = {col_idx: class_label for col_idx, class_label in enumerate(original_classes)}\n",
    "    print(\"Mapping between column indices and original classes:\")\n",
    "    print(mapping)\n",
    "    \n",
    "    print(\"Labels encoded using OneHotEncoder(), labels.shape:\", labels_onehot.shape)\n",
    "    \n",
    "    return labels_onehot, mapping"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T14:33:28.705491500Z",
     "start_time": "2024-06-11T14:33:28.283327300Z"
    }
   },
   "id": "1084a7fd2d625cb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "338661a2c7cca67f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Base CNN model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "302fd355eeea0277"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "from keras.src.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def create_base_cnn_model(feature_width, feature_height, channels, classes):\n",
    "    # Build the CNN model\n",
    "    model = Sequential([\n",
    "        # This layer extracts 64 different 3x3 features\n",
    "        # Typically you’ll see strides of 2×2 as a replacement to max pooling:\n",
    "        Conv2D(64, (3, 3), input_shape=(feature_width, feature_height, channels), padding=\"same\", activation=LeakyReLU(alpha=0.01)),\n",
    "        # BatchNormalization(), # jak to skomentowalem to zniknal overfitting i nawet byl underfitting - moze zostawic jedno ale ogolnie lepiej bym poweidzial, mniej overfiitngu\n",
    "        Dropout(0.25), # after adding it acc was much lower (7%)\n",
    "        \n",
    "        Conv2D(128, (3, 3), strides=(4, 4), padding=\"same\", activation=LeakyReLU(alpha=0.01)),\n",
    "        # BatchNormalization(),\n",
    "        Dropout(0.25), # Dropout helps generalize and not overfit. Neurons from the current layer, with probability p, will randomly disconnect from neurons in the next layer so that the network has to rely on the existing connections.\n",
    "         \n",
    "        Conv2D(128, (3, 3), padding='same', activation=LeakyReLU(alpha=0.01)),\n",
    "        # BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        Conv2D(64, (3, 3), strides=(4, 4), padding='same', activation=LeakyReLU(alpha=0.01)),\n",
    "        # BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "         \n",
    "        Flatten(),\n",
    "        \n",
    "        #So you should add BN after your convolutions and then you should also remove DropOut. It has been studied by many researchers that Dropout is not needed if BN is used and BN performs actually better.\n",
    "        \n",
    "        # # Use ReLU to avoid vanishing gradients problem\n",
    "        # next step -> RELU to LEAKY RELU \n",
    "        # The main advantage of using Leaky ReLU over normal ReLU is that it helps to overcome the \"dying ReLU\" problem. The dying ReLU problem occurs when the neuron stops responding to the input because its output is consistently negative due to the ReLU function. Leaky ReLU solves this problem by allowing a small positive output for negative inputs, which ensures that the neuron remains active even when the input is negative. Additionally, Leaky ReLU can help to improve the performance of deep neural networks by allowing them to learn more complex features and reducing the likelihood of overfitting.\n",
    "        \n",
    "        #Now, think about the chain rule in the backward pass. If the derivative of the slope of the ReLU is of 0, absolutely no learning is performed on the layers below the dead ReLU, because 0 will be multiplied to the accumulated gradient for the weight update. Thus, you can have dead neurons. This problem doesn’t happen with LReLU or ELU for example, they will always have a little slope to allow the gradients to flow on.\n",
    "        \n",
    "\n",
    "        # Dense(512, activation='relu'),\n",
    "        Dense(512, activation=LeakyReLU(alpha=0.01)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Dense(256, activation='relu'),\n",
    "        Dense(256, activation=LeakyReLU(alpha=0.01)),\n",
    "        BatchNormalization(), # adding this normalization (and removing dropout) resulted in model learning faster (on train_acc, val_acc also grows faster but with delay 3-4 epoch)\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # In the Ioffe and Szegedy 2015, the authors state that \"we would like to ensure that for any parameter values, the network always produces activations with the desired distribution\". So the Batch Normalization Layer is actually inserted right after a Conv Layer/Fully Connected Layer, but before feeding into ReLu (or any other kinds of) activation\n",
    "        \n",
    "        # So in summary, the order of using batch normalization and dropout is:\n",
    "        # -> CONV/FC -> BatchNorm -> ReLu(or other activation) -> Dropout -> CONV/FC ->\n",
    "        \n",
    "        \n",
    "        #using a drop outs to avoid stepping to local minimums and labeling all samples to one class\n",
    "    \n",
    "        Dense(classes, activation='softmax'), # in general the best result are produced with softmax\n",
    "        #sigmoid in some cases performs better and prevent stucking in the loss function local minimals\n",
    "        # Categorical Classification: using softmax - multiple output nodes\n",
    "        \n",
    "        #If you have a multi-label classification problem = there is more than one \"right answer\" = the outputs are NOT mutually exclusive, then use a !!!sigmoid function!!! on each raw output independently. The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them. Example: classifying diseases in a chest x-ray image. The image might contain pneumonia, emphysema, and/or cancer, or none of those findings.\n",
    "        \n",
    "        #If you have a multi-class classification problem = there is only one \"right answer\" = the outputs are mutually exclusive, then use a !!!softmax function!!!. The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes. Example: classifying images from the MNIST data set of handwritten digits. A single picture of a digit has only one true identity - the picture cannot be a 7 and an 8 at the same time.\n",
    "    ])\n",
    "    \n",
    "    # Tldr; I’ve seen a good rule-of-thumb is about 14-18x times the model size for memory limits, so for a 10GB card, training your model would max out memory at roughly 540M parameters.\n",
    "    model.summary()\n",
    "    print(f\"[INFO] Using My CNN model.\")\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-09T17:23:22.433537600Z",
     "start_time": "2024-08-09T17:23:22.370636200Z"
    }
   },
   "id": "2b8bef52dfd3fb87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Base CNN ale lepsza"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cafabd3e6ef7fdb1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "from keras.src.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Tldr; I’ve seen a good rule-of-thumb is about 14-18x times the model size for memory limits, so for a 10GB card, training your model would max out memory at roughly 540M parameters.\n",
    "def create_better_base_cnn_model(feature_width, feature_height, channels, classes):\n",
    "    # Build the CNN model\n",
    "    model = Sequential([\n",
    "        # This layer extracts 64 different 3x3 features\n",
    "        # Typically you’ll see strides of 2×2 as a replacement to max pooling:\n",
    "        Conv2D(64, (3, 3), input_shape=(feature_width, feature_height, channels), padding=\"same\", activation=LeakyReLU(alpha=0.1), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Conv2D(64, (3, 3), strides=(4, 4), padding=\"same\", activation=LeakyReLU(alpha=0.1), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.3),\n",
    "         \n",
    "        Conv2D(64, (3, 3), padding='same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Conv2D(64, (3, 3), strides=(4, 4), padding='same', activation=LeakyReLU(alpha=0.1), kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        Dropout(0.4),\n",
    "        \n",
    "        Flatten(),\n",
    "        \n",
    "        Dense(512, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(0.3), \n",
    "        \n",
    "        Dense(256, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(classes, activation='softmax'),\n",
    "    ])\n",
    "\n",
    "    model.summary()\n",
    "    print(f\"[INFO] Using My Better CNN model.\")\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-10T12:50:23.637525100Z",
     "start_time": "2024-08-10T12:50:23.537264200Z"
    }
   },
   "id": "7d1a184362da5d8d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Use Resnet50"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "916e3a88075630d1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-11 16:06:19.872083: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-11 16:06:19.872124: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-11 16:06:19.872248: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-11 16:06:19.888688: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-11 16:06:20.847836: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.src.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "\n",
    "def create_resnet50_model(feature_width, feature_height, classes):\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=(feature_width, feature_height, 3)) #pre proces returns 3 channels output\n",
    "    print(f\"[INFO] Using Resnet50 model with {len(base_model.layers)} base layers\")\n",
    "\n",
    "    # Freeze the layers of the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Add custom layers on top of ResNet50\n",
    "    x = base_model.output\n",
    "    # x = Flatten()(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x) # z 0.5, acc wynislo=\n",
    "    x = tf.keras.layers.Dense(512, activation = 'relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(0.4)(x) # z 0.5, acc wynislo=\n",
    "    predictions = tf.keras.layers.Dense(classes, activation = 'softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "    for layer in model.layers[:175]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    for layer in model.layers[175:]:\n",
    "        layer.trainable = True\n",
    "          \n",
    "    model.summary()\n",
    "    print(f\"[INFO] Using Resnet50 model with {len(base_model.layers)} base layers\")\n",
    "    print(f\"[INFO] Resnet50 customized has a {len(model.layers)} layers\")\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-11T14:06:22.036917700Z",
     "start_time": "2024-08-11T14:06:19.175909Z"
    }
   },
   "id": "eb5569990a4f0aa6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caa667383651887c"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from scipy.interpolate import make_interp_spline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, cohen_kappa_score, ConfusionMatrixDisplay\n",
    "import os\n",
    "\n",
    "def evaluate_model(H, y_pred, y_test, n_of_epochs, show_confusion_matrix=True, interpolate=True, plot_file_name=\"plot.png\"):\n",
    "    print(\"[INFO] Evaluating model...\")\n",
    "\n",
    "    print(f'Type and shape of y_test: {type(y_test)} | {y_test.shape}')\n",
    "    print(f'Type and shape of y_pred: {type(y_pred)} | {y_pred.shape}')\n",
    "    \n",
    "    print('\\nConfusion matrix:') \n",
    "    print(confusion_matrix(y_test, y_pred)) \n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    if show_confusion_matrix:\n",
    "        disp = ConfusionMatrixDisplay(conf_matrix)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        ax.grid(False)\n",
    "        disp.plot(ax=ax)\n",
    "  \n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n",
    "    print(f\"Cohen's Kappa: {cohen_kappa_score(y_test, y_pred)*100:.1f}%\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.1f}%\")\n",
    "    \n",
    "    if H is not None:\n",
    "        # plots the training loss and accuracy\n",
    "        epochs_x = np.arange(0, n_of_epochs)\n",
    "        X_ = np.linspace(epochs_x.min(), epochs_x.max(), 250)\n",
    "        \n",
    "        plt.style.use(\"ggplot\")\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 16))\n",
    "        \n",
    "        # First subplot\n",
    "        plt.subplot(2, 1, 1)\n",
    "        if interpolate:\n",
    "            ax1.plot(X_, return_interpolated_plot(H.history[\"loss\"][:n_of_epochs], epochs_x, X_), label=\"train_loss\")\n",
    "            ax1.plot(X_, return_interpolated_plot(H.history[\"val_loss\"][:n_of_epochs], epochs_x, X_), label=\"val_loss\")\n",
    "        else:\n",
    "            ax1.plot(epochs_x, H.history[\"loss\"][:n_of_epochs], label=\"train_loss\")\n",
    "            ax1.plot(epochs_x, H.history[\"val_loss\"][:n_of_epochs], label=\"val_loss\")\n",
    "        ax1.set_title(\"Training and Validation Loss on Dataset\")\n",
    "        ax1.set_xlabel(\"Epoch #\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.legend(loc=\"lower left\")\n",
    "        \n",
    "        # Second subplot\n",
    "        plt.subplot(2, 1, 2)\n",
    "        if interpolate:\n",
    "            ax2.plot(X_, return_interpolated_plot(H.history[\"accuracy\"][:n_of_epochs], epochs_x, X_), label=\"train_acc\")\n",
    "            ax2.plot(X_, return_interpolated_plot(H.history[\"val_accuracy\"][:n_of_epochs], epochs_x, X_), label=\"val_acc\")\n",
    "        else:\n",
    "            ax2.plot(epochs_x, H.history[\"accuracy\"][:n_of_epochs], label=\"train_acc\")\n",
    "            ax2.plot(epochs_x, H.history[\"val_accuracy\"][:n_of_epochs], label=\"val_acc\")\n",
    "        ax2.set_title(\"Training and Validation Accuracy on Dataset\")\n",
    "        ax2.set_xlabel(\"Epoch #\")\n",
    "        ax2.set_ylabel(\"Accuracy\")\n",
    "        ax2.legend(loc=\"lower left\")\n",
    "        \n",
    "        print(f\"[INFO] Saving training/validation plot as a {plot_file_name}-{accuracy_score(y_test, y_pred)*100:.2f}acc.png\")\n",
    "\n",
    "        plt.savefig(f\"{plot_file_name}-{accuracy_score(y_test, y_pred)*100:.1f}acc.png\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    print(\"[INFO] Finished model evaluation\")    \n",
    "    \n",
    "def return_interpolated_plot(y, x, X_):\n",
    "    X_Y_Spline = make_interp_spline(x , y)\n",
    "    Y_ = X_Y_Spline(X_)\n",
    " \n",
    "    return Y_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-17T18:39:12.494928400Z",
     "start_time": "2024-06-17T18:39:12.481878100Z"
    }
   },
   "id": "22805e6953758f2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
